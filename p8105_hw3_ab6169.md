p8105_hw3_ab6169
================
Amrutha Banda
2025-10-03

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.2     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1
    ## ✔ purrr     1.0.4     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(janitor)
```

    ## 
    ## Attaching package: 'janitor'
    ## 
    ## The following objects are masked from 'package:stats':
    ## 
    ##     chisq.test, fisher.test

``` r
library(lubridate)
```

## Problem 1

``` r
library(p8105.datasets)
data("instacart")
```

Short Description:

``` r
aisles_summary= 
  instacart |> 
  count(aisle, sort=TRUE)

aisles_summary |> 
  head(10) #This gives me a table that shows me top 10 aisles with the most ordered items 
```

    ## # A tibble: 10 × 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635

Comments:

Making a Plot n\>10,000

``` r
aisles_summary |> 
  filter(n > 10000) |> 
  mutate(aisle = fct_reorder(aisle, n)) |> #shows me aisles based on n (size)
  ggplot(aes(x = aisle, y = n, fill = aisle)) +
  geom_col(show.legend = FALSE) +
  coord_flip() ##flips the long aisle names
```

![](p8105_hw3_ab6169_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

``` r
  labs(
    title = "Number of Items Ordered by Aisle (>10,000 orders)",
    x = "Aisle",
    y = "Number of Items Ordered"
  )
```

    ## $x
    ## [1] "Aisle"
    ## 
    ## $y
    ## [1] "Number of Items Ordered"
    ## 
    ## $title
    ## [1] "Number of Items Ordered by Aisle (>10,000 orders)"
    ## 
    ## attr(,"class")
    ## [1] "labels"

Comments:

Table of 3 most popular items

``` r
instacart |> 
  filter(aisle %in% 
    c("baking ingredients", "dog food care", "packaged vegetables fruits")) |> 
  group_by(aisle, product_name) |>
  summarize(n = n()) |> 
  slice_max(order_by = n, n = 3) |> 
  knitr::kable(digits = 0)
```

    ## `summarise()` has grouped output by 'aisle'. You can override using the
    ## `.groups` argument.

| aisle | product_name | n |
|:---|:---|---:|
| baking ingredients | Light Brown Sugar | 499 |
| baking ingredients | Pure Baking Soda | 387 |
| baking ingredients | Cane Sugar | 336 |
| dog food care | Snack Sticks Chicken & Rice Recipe Dog Treats | 30 |
| dog food care | Organix Chicken & Brown Rice Recipe | 28 |
| dog food care | Small Dog Biscuits | 26 |
| packaged vegetables fruits | Organic Baby Spinach | 9784 |
| packaged vegetables fruits | Organic Raspberries | 5546 |
| packaged vegetables fruits | Organic Blueberries | 4966 |

Comments:

Table of Mean Hour of the Day

``` r
instacart |> 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) |> 
  group_by(product_name, order_dow) |> 
  summarize(mean_hour = mean(order_hour_of_day)) |> 
  mutate(order_dow = recode(order_dow,
  `0` = "Sun",
  `1` = "Mon", 
  `2` = "Tue",
  `3` = "Wed",
  `4` = "Thu", 
  `5` = "Fri",
  `6` = "Sat")) |> 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour ) |> 
  knitr::kable(digits=2) 
```

    ## `summarise()` has grouped output by 'product_name'. You can override using the
    ## `.groups` argument.

| product_name     |   Sun |   Mon |   Tue |   Wed |   Thu |   Fri |   Sat |
|:-----------------|------:|------:|------:|------:|------:|------:|------:|
| Coffee Ice Cream | 13.77 | 14.32 | 15.38 | 15.32 | 15.22 | 12.26 | 13.83 |
| Pink Lady Apples | 13.44 | 11.36 | 11.70 | 14.25 | 11.55 | 12.78 | 11.94 |

Comments:

## Problem 2

``` r
zipcodes_df= 
  read_csv("data/zipcodes.csv", na = c("NA", ".", "")) |>
  janitor::clean_names() |> 
  select(-state_fips, -file_date) |> 
  mutate(
    borough=recode(county,
          "Kings"= "Brooklyn",
          "New York"= "Manhattan",
          "Richmond"= "Staten Island") 
  ) |> 
  select(borough, county, zip_code, neighborhood)
```

    ## Rows: 322 Columns: 7
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (4): County, County Code, File Date, Neighborhood
    ## dbl (3): State FIPS, County FIPS, ZipCode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
zori_df = 
  read_csv("data/zori.csv", na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  pivot_longer(
    cols = starts_with("x20"),
    names_to = "date",
    values_to = "zori"
  ) |> 
  rename(
    zip_code= region_name) |> 
  mutate(county_name= str_remove(county_name, " County")) |> 
  rename(county= county_name) |> 
  select(-city, -metro, -region_type, -state_name, -state, ) |> 
  mutate(date = sub("^x", "", date))
```

    ## Rows: 149 Columns: 125
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr   (6): RegionType, StateName, State, City, Metro, CountyName
    ## dbl (119): RegionID, SizeRank, RegionName, 2015-01-31, 2015-02-28, 2015-03-3...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
#Joined DataFrame
mergezip_df=
  left_join(zipcodes_df, zori_df, by = c("zip_code","county"))
```

``` r
##Made a column for just year 
mergezip_df = 
  mergezip_df |> 
  mutate(year = as.numeric(substr(date, 1, 4)))
```

``` r
borough_year_df = 
  mergezip_df |> 
  group_by(borough, year) |> 
  summarize(
    mean_zori = mean(zori, na.rm = TRUE),
    .groups = "drop"
  ) |> 
  arrange(borough, year)
```

Reader Friendly Table

``` r
borough_year_df |> 
  knitr::kable(
    digits = 0,
    col.names = c("Borough", "Year", "Average ZORI (USD)")
  )
```

| Borough       | Year | Average ZORI (USD) |
|:--------------|-----:|-------------------:|
| Bronx         | 2015 |               1760 |
| Bronx         | 2016 |               1520 |
| Bronx         | 2017 |               1544 |
| Bronx         | 2018 |               1639 |
| Bronx         | 2019 |               1706 |
| Bronx         | 2020 |               1811 |
| Bronx         | 2021 |               1858 |
| Bronx         | 2022 |               2054 |
| Bronx         | 2023 |               2285 |
| Bronx         | 2024 |               2497 |
| Bronx         |   NA |                NaN |
| Brooklyn      | 2015 |               2493 |
| Brooklyn      | 2016 |               2520 |
| Brooklyn      | 2017 |               2546 |
| Brooklyn      | 2018 |               2547 |
| Brooklyn      | 2019 |               2631 |
| Brooklyn      | 2020 |               2555 |
| Brooklyn      | 2021 |               2550 |
| Brooklyn      | 2022 |               2868 |
| Brooklyn      | 2023 |               3015 |
| Brooklyn      | 2024 |               3127 |
| Brooklyn      |   NA |                NaN |
| Manhattan     | 2015 |               3022 |
| Manhattan     | 2016 |               3039 |
| Manhattan     | 2017 |               3134 |
| Manhattan     | 2018 |               3184 |
| Manhattan     | 2019 |               3310 |
| Manhattan     | 2020 |               3107 |
| Manhattan     | 2021 |               3137 |
| Manhattan     | 2022 |               3778 |
| Manhattan     | 2023 |               3933 |
| Manhattan     | 2024 |               4078 |
| Manhattan     |   NA |                NaN |
| Queens        | 2015 |               2215 |
| Queens        | 2016 |               2272 |
| Queens        | 2017 |               2263 |
| Queens        | 2018 |               2292 |
| Queens        | 2019 |               2388 |
| Queens        | 2020 |               2316 |
| Queens        | 2021 |               2211 |
| Queens        | 2022 |               2406 |
| Queens        | 2023 |               2562 |
| Queens        | 2024 |               2694 |
| Queens        |   NA |                NaN |
| Staten Island | 2015 |                NaN |
| Staten Island | 2016 |                NaN |
| Staten Island | 2017 |                NaN |
| Staten Island | 2018 |                NaN |
| Staten Island | 2019 |                NaN |
| Staten Island | 2020 |               1978 |
| Staten Island | 2021 |               2045 |
| Staten Island | 2022 |               2147 |
| Staten Island | 2023 |               2333 |
| Staten Island | 2024 |               2536 |
| Staten Island |   NA |                NaN |
